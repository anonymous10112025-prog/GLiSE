"""
search_execute_split.py
Script 2: execution of queries from one or two CSVs generated by queries_generate_split.py.
Produces a single CSV: search_results_combined.csv

This script now uses the refactored provider architecture with polymorphic search() methods.
"""

import os
import sys
import csv
import time
import argparse
from typing import List, Dict, Optional

from model.providers import get_provider, get_all_providers
from model.providers.google_provider import GoogleProvider
from model.Settings import get_settings


# Load settings
settings = get_settings()
MAX_RESULTS = settings.get('MAX_RESULTS_PER_QUERY_DEFAULT', 50)
SLEEP_BETWEEN = settings.get('SLEEP_BETWEEN', 1.0)

def read_csv_rows(path: str) -> List[Dict]:
    """Read CSV file and return list of row dictionaries."""
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            rows.append(row)
    return rows


def parse_platform(row_platform: str) -> str:
    """
    Parse platform name from CSV row and return normalized provider ID.
    
    Args:
        row_platform: Platform name from CSV
        
    Returns:
        Normalized provider ID
    """
    p = (row_platform or "").lower()
    
    if "stack" in p:
        return "so"
    if "github" in p and "code" in p:
        return "gh_code"
    if "github" in p and ("issues" in p or "issue" in p):
        return "gh_issues"
    if "github" in p and ("repo" in p or "repository" in p):
        return "gh_repos"
    if "google" in p:
        return "google"
    
    return "google"  # Default to Google


def exec_row(row: Dict) -> List[Dict]:
    """
    Execute a search for a single CSV row using the appropriate provider.
    
    Args:
        row: Dictionary containing Platform and Query keys
        
    Returns:
        List of search result dictionaries
    """
    provider_id = parse_platform(row.get("Platform", ""))
    query = row.get("Query", "")
    
    if not query:
        return []
    
    try:
        # Get the provider instance
        provider = get_provider(provider_id)
        
        # Execute the search
        results = provider.search(query, MAX_RESULTS)
        
        # If primary search fails or returns no results, fallback to Google with site: prefix
        if not results and provider_id != "google":
            google_provider = GoogleProvider()
            
            # Add site: prefix for fallback searches
            site_prefixes = {
                "gh_code": "site:github.com",
                "gh_issues": "site:github.com",
                "gh_repos": "site:github.com",
                "so": "site:stackoverflow.com",
            }
            
            site_prefix = site_prefixes.get(provider_id, "")
            fallback_query = f"{site_prefix} {query}".strip()
            results = google_provider.search(fallback_query, MAX_RESULTS)
        
        # Add a small delay between requests
        time.sleep(SLEEP_BETWEEN)
        
        return results
    
    except Exception as e:
        print(f"Error executing search for {provider_id}: {e}", file=sys.stderr)
        return []


def dedupe_by_url(items: List[Dict]) -> List[Dict]:
    """Remove duplicate results based on URL."""
    seen = set()
    out = []
    for it in items:
        url = it.get("url")
        if url and url not in seen:
            seen.add(url)
            out.append(it)
    return out

def parse_args():
    """Parse command line arguments."""
    ap = argparse.ArgumentParser(description="Execute queries from CSV and aggregate results")
    ap.add_argument("--code-csv", type=str, default=None, help="Path to queries_code.csv")
    ap.add_argument("--google-csv", type=str, default=None, help="Path to queries_google.csv")
    ap.add_argument("--output", type=str, default="search_results_combined.csv", help="Output CSV file")
    return ap.parse_args()


def main():
    """Main entry point for CLI execution."""
    args = parse_args()
    
    # Read CSV rows
    rows = []
    if args.code_csv and os.path.exists(args.code_csv):
        rows.extend(read_csv_rows(args.code_csv))
    if args.google_csv and os.path.exists(args.google_csv):
        rows.extend(read_csv_rows(args.google_csv))
    
    if not rows:
        print("No valid CSV provided. Pass --code-csv and/or --google-csv.", file=sys.stderr)
        sys.exit(2)
    
    # Execute searches
    all_results = []
    for idx, row in enumerate(rows):
        print(f"Processing query {idx + 1}/{len(rows)}: {row.get('Query', '')[:50]}...")
        res = exec_row(row)
        all_results.extend(res)
    
    # Deduplicate results
    all_results = dedupe_by_url(all_results)
    
    # Write results to CSV
    with open(args.output, "w", newline="", encoding="utf-8") as f:
        fieldnames = [
            "source", "title", "url", "snippet", "search_query", 
            "repo", "state", "comments", "is_answered", "score",
            "stargazers_count", "language", "readme"
        ]
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader()
        for it in all_results:
            w.writerow({k: it.get(k, "") for k in fieldnames})
    
    print(f"\nâœ“ Saved {args.output} with {len(all_results)} results.")


if __name__ == "__main__":
    main()
